{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news_classification_with_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpRvhWjOQ6SVwd+m97CRh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VGODIE/ML_kaggle_competitions/blob/master/news_classification_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFdmM8SY1PNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import re\n",
        "import copy\n",
        "import traceback\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apVh77nA2Woc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_source = fetch_20newsgroups(subset=\"train\")\n",
        "test_source = fetch_20newsgroups(subset=\"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-UvhTee2pbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
        "\n",
        "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
        "  txt = txt.lower()\n",
        "  all_tokens = TOKEN_RE.findall(txt)\n",
        "  return [token for token in all_tokens if len(token) >= min_token_size]\n",
        "\n",
        "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex):\n",
        "  return [tokenizer(text) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mUcR7DX37ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(\" \").join(tokenize_corpus([train_source[\"data\"][0]])[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucixv6YN5u_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokenized = tokenize_corpus(train_source[\"data\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5b1SCx8MpI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tokenized = tokenize_corpus(test_source[\"data\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4oaG_9r53kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(tokenized_texts,\n",
        "                     max_size=1000000,\n",
        "                     max_doc_freq=0.8,\n",
        "                     min_count=5,\n",
        "                     pad_word=None):\n",
        "  word_counts = collections.defaultdict(int)\n",
        "  doc_n = 0\n",
        "\n",
        "  for txt in tokenized_texts:\n",
        "    doc_n += 1\n",
        "    unique_text_tokens = set(txt)\n",
        "    for token in unique_text_tokens:\n",
        "      word_counts[token] += 1\n",
        "  \n",
        "  word_counts = {word: cnt for word, cnt in word_counts.items()\n",
        "                if cnt >= min_count and cnt/doc_n <= max_doc_freq}\n",
        "  sorted_word_counts = sorted(word_counts.items(),\n",
        "                              reverse=True,\n",
        "                              key=lambda pair: pair[1])\n",
        "  if pad_word is not None:\n",
        "    sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
        "\n",
        "  if len(word_counts) > max_size:\n",
        "    sorted_word_counts = sorted_word_counts[:max_size]\n",
        "\n",
        "  #build vocab\n",
        "  word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
        "  word2freq = np.array([cnt/doc_n for _, cnt in sorted_word_counts], dtype=\"float32\")\n",
        "\n",
        "  return word2id, word2freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxxR-537GBj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "\n",
        "def vectorize_texts(tokenized_texts, word2id,word2freq,  mode=\"tfidf\", scale=True):\n",
        "  result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "\n",
        "  for i, text in enumerate(tokenized_texts):\n",
        "    for token in text:\n",
        "      if token in word2id:\n",
        "        result[i, word2id[token]] += 1\n",
        "  result = result.tocsr()\n",
        "  result = result.multiply(1/result.sum(1))\n",
        "  result = result.multiply(1/word2freq)\n",
        "\n",
        "  if scale:\n",
        "    result.tocsr()\n",
        "    result -= result.min()\n",
        "    result /= (result.max() + 1e-6)\n",
        "  \n",
        "  return result.tocsr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyfj-mX4-WK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized,\n",
        "                                             max_doc_freq=MAX_DF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hXxTaSW-KKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = \"tfidf\"\n",
        "\n",
        "train_vectorized = vectorize_texts(train_tokenized, vocabulary, word_doc_freq)\n",
        "test_vectorized = vectorize_texts(test_tokenized, vocabulary, word_doc_freq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPHVgYz0tqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNIQUE_WORDS = train_vectorized.shape[1]\n",
        "UNIQUE_LABELS = len(set(train_source[\"target\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSyqICNN3qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SparseFeautureDataset(Dataset):\n",
        "  def __init__(self, features, targets):\n",
        "    self.features = features\n",
        "    self.targets = targets\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "    cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
        "    cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
        "    return cur_features, cur_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx46s3xmbQWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = SparseFeautureDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyeBE1QzL3J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SparseFeautureDataset(train_vectorized, train_source[\"target\"])\n",
        "test_dataset = SparseFeautureDataset(test_vectorized, test_source[\"target\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs5qf8SONItS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def lr_scheduler(optim):\n",
        "  return torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\n",
        "                                                    patience=5,\n",
        "                                                    factor=0.5,\n",
        "                                                    verbose=True)\n",
        "\n",
        "def train_eval_loop(model, train_dataset, val_dataset, loss_func,\n",
        "                    lr=1e-4, epochs=10, batch_size=32,\n",
        "                    max_batches_per_epoch_train = 1000,\n",
        "                    max_batches_per_epoch_val = 1000,\n",
        "                    early_stopping_patience=10, l2_reg_alpha = 0,\n",
        "                    data_loader=DataLoader, optimizer=None, shuffle_train=True,\n",
        "                    lr_scheduler=None, device=\"cuda\"):\n",
        "  device = torch.device(device)\n",
        "  model.to(device)\n",
        "\n",
        "  if optimizer is None:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  train_dataloader = data_loader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  val_dataloader = data_loader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  best_val_loss = float('inf')\n",
        "  best_epoch_i = 0\n",
        "  best_model = copy.deepcopy(model)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    try:\n",
        "      epoch_start = datetime.datetime.now()\n",
        "      print('Epoch {}'.format(epoch))\n",
        "\n",
        "      model.train()\n",
        "      mean_train_loss = 0\n",
        "      train_batches = 0\n",
        "      for step, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "        if step > max_batches_per_epoch_train:\n",
        "          break\n",
        "        \n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        pred = model(batch_x)\n",
        "        loss = loss_func(pred, batch_y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_train_loss += float(loss)\n",
        "        train_batches += 1\n",
        "\n",
        "      mean_train_loss /= train_batches\n",
        "      print(\"Epoch: {} iterations, {:0.2f} sec\".format(epoch, (datetime.datetime.now()\n",
        "        - epoch_start).total_seconds()))\n",
        "      print(\"Mean train loss =\", mean_train_loss)\n",
        "\n",
        "      model.eval()\n",
        "      mean_val_loss = 0\n",
        "      val_batches = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for step, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "          if step > max_batches_per_epoch_val:\n",
        "            break\n",
        "        \n",
        "          batch_x = batch_x.to(device)\n",
        "          batch_y = batch_y.to(device)\n",
        "\n",
        "          pred = model(batch_x)\n",
        "          loss = loss_func(pred, batch_y)\n",
        "\n",
        "          mean_val_loss += float(loss)\n",
        "          val_batches += 1\n",
        "\n",
        "        mean_val_loss /= val_batches\n",
        "        print(\"Mean val loss =\", mean_val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "          best_epoch_i = step\n",
        "          best_val_loss = mean_val_loss\n",
        "          best_model = copy.deepcopy(model)\n",
        "          print(\"New best model ! PUSHKA\")\n",
        "        elif step - best_epoch_i > early_stopping_patience:\n",
        "          print(\"Model hasn't evolved within last {} epochs, training stops, wayaaa\".format(\n",
        "              early_stopping_patience\n",
        "            ))\n",
        "          break\n",
        "      print()\n",
        "          \n",
        "        #if lr_scheduler is not None:\n",
        "         # lr_scheduler.step(mean_val_loss)\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Keyboard interruption\")\n",
        "      break\n",
        "    except Exception as ex:\n",
        "      print(\"Failure during training: {}\\n{}\".format(ex, traceback.format_exc()))\n",
        "      break\n",
        "  return best_val_loss, best_model     \n",
        "\n",
        "def lr_scheduler(optim):\n",
        "  return torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\n",
        "                                                    patience=5,\n",
        "                                                    factor=0.5,\n",
        "                                                    verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jZ2KFkM0h0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Linear(UNIQUE_WORDS, UNIQUE_LABELS)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=test_dataset,\n",
        "    loss_func=F.cross_entropy,\n",
        "    lr=1e-1,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    l2_reg_alpha=0,\n",
        "    lr_scheduler=lr_scheduler\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sERcgtzaAMxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}